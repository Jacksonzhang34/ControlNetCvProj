LLSUB_RANK=1: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
LLSUB_RANK=1:   rank_zero_warn(
LLSUB_RANK=1: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 model_training.py --prompt prompt1_longest ...
LLSUB_RANK=1:   rank_zero_warn(
LLSUB_RANK=1: Using 16bit Automatic Mixed Precision (AMP)
LLSUB_RANK=1: GPU available: True (cuda), used: True
LLSUB_RANK=1: TPU available: False, using: 0 TPU cores
LLSUB_RANK=1: IPU available: False, using: 0 IPUs
LLSUB_RANK=1: HPU available: False, using: 0 HPUs
LLSUB_RANK=1: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 model_training.py --prompt prompt1_longest ...
LLSUB_RANK=1:   rank_zero_warn(
LLSUB_RANK=1: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-2766bb39-cadf-bc5a-daf2-5f5c659bc4e9]
LLSUB_RANK=1: 
LLSUB_RANK=1:   | Name              | Type                   | Params
LLSUB_RANK=1: -------------------------------------------------------------
LLSUB_RANK=1: 0 | model             | DiffusionWrapper       | 865 M 
LLSUB_RANK=1: 1 | first_stage_model | AutoencoderKL          | 83.7 M
LLSUB_RANK=1: 2 | cond_stage_model  | FrozenOpenCLIPEmbedder | 354 M 
LLSUB_RANK=1: 3 | control_model     | ControlNet             | 364 M 
LLSUB_RANK=1: -------------------------------------------------------------
LLSUB_RANK=1: 1.2 B     Trainable params
LLSUB_RANK=1: 437 M     Non-trainable params
LLSUB_RANK=1: 1.7 B     Total params
LLSUB_RANK=1: 6,671.302 Total estimated model params size (MB)
LLSUB_RANK=1: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/loggers/csv_logs.py:188: UserWarning: Experiment logs directory csv_log/prompt1_longest/version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
LLSUB_RANK=1:   rank_zero_warn(
LLSUB_RANK=1: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
LLSUB_RANK=1:   rank_zero_warn(
LLSUB_RANK=1: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
LLSUB_RANK=1:   warning_cache.warn(
LLSUB_RANK=1: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
LLSUB_RANK=1:   rank_zero_warn(
LLSUB_RANK=1: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('LLSUB_RANK=0: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
LLSUB_RANK=0:   rank_zero_warn(
LLSUB_RANK=0: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 model_training.py --prompt prompt1_random ...
LLSUB_RANK=0:   rank_zero_warn(
LLSUB_RANK=0: Using 16bit Automatic Mixed Precision (AMP)
LLSUB_RANK=0: GPU available: True (cuda), used: True
LLSUB_RANK=0: TPU available: False, using: 0 TPU cores
LLSUB_RANK=0: IPU available: False, using: 0 IPUs
LLSUB_RANK=0: HPU available: False, using: 0 HPUs
LLSUB_RANK=0: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 model_training.py --prompt prompt1_random ...
LLSUB_RANK=0:   rank_zero_warn(
LLSUB_RANK=0: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-490506cf-cfe5-7782-2b05-28913e093079]
LLSUB_RANK=0: 
LLSUB_RANK=0:   | Name              | Type                   | Params
LLSUB_RANK=0: -------------------------------------------------------------
LLSUB_RANK=0: 0 | model             | DiffusionWrapper       | 865 M 
LLSUB_RANK=0: 1 | first_stage_model | AutoencoderKL          | 83.7 M
LLSUB_RANK=0: 2 | cond_stage_model  | FrozenOpenCLIPEmbedder | 354 M 
LLSUB_RANK=0: 3 | control_model     | ControlNet             | 364 M 
LLSUB_RANK=0: -------------------------------------------------------------
LLSUB_RANK=0: 1.2 B     Trainable params
LLSUB_RANK=0: 437 M     Non-trainable params
LLSUB_RANK=0: 1.7 B     Total params
LLSUB_RANK=0: 6,671.302 Total estimated model params size (MB)
LLSUB_RANK=0: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/loggers/csv_logs.py:188: UserWarning: Experiment logs directory csv_log/prompt1_random/version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
LLSUB_RANK=0:   rank_zero_warn(
LLSUB_RANK=0: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
LLSUB_RANK=0:   rank_zero_warn(
LLSUB_RANK=0: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
LLSUB_RANK=0:   warning_cache.warn(
LLSUB_RANK=0: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
LLSUB_RANK=0:   rank_zero_warn(
LLSUB_RANK=0: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('glo