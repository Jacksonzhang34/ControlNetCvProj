LLSUB_RANK=3: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
LLSUB_RANK=3:   rank_zero_warn(
LLSUB_RANK=3: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 model_training.py --prompt prompt1_gpt_best ...
LLSUB_RANK=3:   rank_zero_warn(
LLSUB_RANK=3: Using 16bit Automatic Mixed Precision (AMP)
LLSUB_RANK=3: GPU available: True (cuda), used: True
LLSUB_RANK=3: TPU available: False, using: 0 TPU cores
LLSUB_RANK=3: IPU available: False, using: 0 IPUs
LLSUB_RANK=3: HPU available: False, using: 0 HPUs
LLSUB_RANK=3: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 model_training.py --prompt prompt1_gpt_best ...
LLSUB_RANK=3:   rank_zero_warn(
LLSUB_RANK=3: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-2766bb39-cadf-bc5a-daf2-5f5c659bc4e9]
LLSUB_RANK=3: 
LLSUB_RANK=3:   | Name              | Type                   | Params
LLSUB_RANK=3: -------------------------------------------------------------
LLSUB_RANK=3: 0 | model             | DiffusionWrapper       | 865 M 
LLSUB_RANK=3: 1 | first_stage_model | AutoencoderKL          | 83.7 M
LLSUB_RANK=3: 2 | cond_stage_model  | FrozenOpenCLIPEmbedder | 354 M 
LLSUB_RANK=3: 3 | control_model     | ControlNet             | 364 M 
LLSUB_RANK=3: -------------------------------------------------------------
LLSUB_RANK=3: 1.2 B     Trainable params
LLSUB_RANK=3: 437 M     Non-trainable params
LLSUB_RANK=3: 1.7 B     Total params
LLSUB_RANK=3: 6,671.302 Total estimated model params size (MB)
LLSUB_RANK=3: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/loggers/csv_logs.py:188: UserWarning: Experiment logs directory csv_log/prompt1_gpt_best/version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
LLSUB_RANK=3:   rank_zero_warn(
LLSUB_RANK=3: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
LLSUB_RANK=3:   rank_zero_warn(
LLSUB_RANK=3: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
LLSUB_RANK=3:   warning_cache.warn(
LLSUB_RANK=3: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
LLSUB_RANK=3:   rank_zero_warn(
LLSUB_RANK=3: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.loLLSUB_RANK=2: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
LLSUB_RANK=2:   rank_zero_warn(
LLSUB_RANK=2: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 model_training.py --prompt prompt1_tf_idf ...
LLSUB_RANK=2:   rank_zero_warn(
LLSUB_RANK=2: Using 16bit Automatic Mixed Precision (AMP)
LLSUB_RANK=2: GPU available: True (cuda), used: True
LLSUB_RANK=2: TPU available: False, using: 0 TPU cores
LLSUB_RANK=2: IPU available: False, using: 0 IPUs
LLSUB_RANK=2: HPU available: False, using: 0 HPUs
LLSUB_RANK=2: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 model_training.py --prompt prompt1_tf_idf ...
LLSUB_RANK=2:   rank_zero_warn(
LLSUB_RANK=2: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-490506cf-cfe5-7782-2b05-28913e093079]
LLSUB_RANK=2: 
LLSUB_RANK=2:   | Name              | Type                   | Params
LLSUB_RANK=2: -------------------------------------------------------------
LLSUB_RANK=2: 0 | model             | DiffusionWrapper       | 865 M 
LLSUB_RANK=2: 1 | first_stage_model | AutoencoderKL          | 83.7 M
LLSUB_RANK=2: 2 | cond_stage_model  | FrozenOpenCLIPEmbedder | 354 M 
LLSUB_RANK=2: 3 | control_model     | ControlNet             | 364 M 
LLSUB_RANK=2: -------------------------------------------------------------
LLSUB_RANK=2: 1.2 B     Trainable params
LLSUB_RANK=2: 437 M     Non-trainable params
LLSUB_RANK=2: 1.7 B     Total params
LLSUB_RANK=2: 6,671.302 Total estimated model params size (MB)
LLSUB_RANK=2: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/loggers/csv_logs.py:188: UserWarning: Experiment logs directory csv_log/prompt1_tf_idf/version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
LLSUB_RANK=2:   rank_zero_warn(
LLSUB_RANK=2: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
LLSUB_RANK=2:   rank_zero_warn(
LLSUB_RANK=2: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
LLSUB_RANK=2:   warning_cache.warn(
LLSUB_RANK=2: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
LLSUB_RANK=2:   rank_zero_warn(
LLSUB_RANK=2: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('gloLLSUB_RANK=6: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
LLSUB_RANK=6:   rank_zero_warn(
LLSUB_RANK=6: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 model_training.py --prompt prompt2_cat5 ...
LLSUB_RANK=6:   rank_zero_warn(
LLSUB_RANK=6: Using 16bit Automatic Mixed Precision (AMP)
LLSUB_RANK=6: GPU available: True (cuda), used: True
LLSUB_RANK=6: TPU available: False, using: 0 TPU cores
LLSUB_RANK=6: IPU available: False, using: 0 IPUs
LLSUB_RANK=6: HPU available: False, using: 0 HPUs
LLSUB_RANK=6: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 model_training.py --prompt prompt2_cat5 ...
LLSUB_RANK=6:   rank_zero_warn(
LLSUB_RANK=6: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-42ce0213-d4cd-d418-0188-8a9184079632]
LLSUB_RANK=6: 
LLSUB_RANK=6:   | Name              | Type                   | Params
LLSUB_RANK=6: -------------------------------------------------------------
LLSUB_RANK=6: 0 | model             | DiffusionWrapper       | 865 M 
LLSUB_RANK=6: 1 | first_stage_model | AutoencoderKL          | 83.7 M
LLSUB_RANK=6: 2 | cond_stage_model  | FrozenOpenCLIPEmbedder | 354 M 
LLSUB_RANK=6: 3 | control_model     | ControlNet             | 364 M 
LLSUB_RANK=6: -------------------------------------------------------------
LLSUB_RANK=6: 1.2 B     Trainable params
LLSUB_RANK=6: 437 M     Non-trainable params
LLSUB_RANK=6: 1.7 B     Total params
LLSUB_RANK=6: 6,671.302 Total estimated model params size (MB)
LLSUB_RANK=6: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/loggers/csv_logs.py:188: UserWarning: Experiment logs directory csv_log/prompt2_cat5/version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
LLSUB_RANK=6:   rank_zero_warn(
LLSUB_RANK=6: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
LLSUB_RANK=6:   rank_zero_warn(
LLSUB_RANK=6: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
LLSUB_RANK=6:   warning_cache.warn(
LLSUB_RANK=6: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
LLSUB_RANK=6:   rank_zero_warn(
LLSUB_RANK=6: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('global_stLLSUB_RANK=7: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
LLSUB_RANK=7:   rank_zero_warn(
LLSUB_RANK=7: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 model_training.py --prompt prompt2_sum ...
LLSUB_RANK=7:   rank_zero_warn(
LLSUB_RANK=7: Using 16bit Automatic Mixed Precision (AMP)
LLSUB_RANK=7: GPU available: True (cuda), used: True
LLSUB_RANK=7: TPU available: False, using: 0 TPU cores
LLSUB_RANK=7: IPU available: False, using: 0 IPUs
LLSUB_RANK=7: HPU available: False, using: 0 HPUs
LLSUB_RANK=7: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 model_training.py --prompt prompt2_sum ...
LLSUB_RANK=7:   rank_zero_warn(
LLSUB_RANK=7: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-75c5ba62-3e56-95b2-43d9-dfc60b12d658]
LLSUB_RANK=7: 
LLSUB_RANK=7:   | Name              | Type                   | Params
LLSUB_RANK=7: -------------------------------------------------------------
LLSUB_RANK=7: 0 | model             | DiffusionWrapper       | 865 M 
LLSUB_RANK=7: 1 | first_stage_model | AutoencoderKL          | 83.7 M
LLSUB_RANK=7: 2 | cond_stage_model  | FrozenOpenCLIPEmbedder | 354 M 
LLSUB_RANK=7: 3 | control_model     | ControlNet             | 364 M 
LLSUB_RANK=7: -------------------------------------------------------------
LLSUB_RANK=7: 1.2 B     Trainable params
LLSUB_RANK=7: 437 M     Non-trainable params
LLSUB_RANK=7: 1.7 B     Total params
LLSUB_RANK=7: 6,671.302 Total estimated model params size (MB)
LLSUB_RANK=7: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/loggers/csv_logs.py:188: UserWarning: Experiment logs directory csv_log/prompt2_sum/version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
LLSUB_RANK=7:   rank_zero_warn(
LLSUB_RANK=7: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
LLSUB_RANK=7:   rank_zero_warn(
LLSUB_RANK=7: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
LLSUB_RANK=7:   warning_cache.warn(
LLSUB_RANK=7: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
LLSUB_RANK=7:   rank_zero_warn(
LLSUB_RANK=7: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('global_step'LLSUB_RANK=4: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
LLSUB_RANK=4:   rank_zero_warn(
LLSUB_RANK=4: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 model_training.py --prompt prompt2_cat2 ...
LLSUB_RANK=4:   rank_zero_warn(
LLSUB_RANK=4: Using 16bit Automatic Mixed Precision (AMP)
LLSUB_RANK=4: GPU available: True (cuda), used: True
LLSUB_RANK=4: TPU available: False, using: 0 TPU cores
LLSUB_RANK=4: IPU available: False, using: 0 IPUs
LLSUB_RANK=4: HPU available: False, using: 0 HPUs
LLSUB_RANK=4: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 model_training.py --prompt prompt2_cat2 ...
LLSUB_RANK=4:   rank_zero_warn(
LLSUB_RANK=4: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-e763804c-fc8c-4beb-bf2e-80cf2d9f36fd]
LLSUB_RANK=4: 
LLSUB_RANK=4:   | Name              | Type                   | Params
LLSUB_RANK=4: -------------------------------------------------------------
LLSUB_RANK=4: 0 | model             | DiffusionWrapper       | 865 M 
LLSUB_RANK=4: 1 | first_stage_model | AutoencoderKL          | 83.7 M
LLSUB_RANK=4: 2 | cond_stage_model  | FrozenOpenCLIPEmbedder | 354 M 
LLSUB_RANK=4: 3 | control_model     | ControlNet             | 364 M 
LLSUB_RANK=4: -------------------------------------------------------------
LLSUB_RANK=4: 1.2 B     Trainable params
LLSUB_RANK=4: 437 M     Non-trainable params
LLSUB_RANK=4: 1.7 B     Total params
LLSUB_RANK=4: 6,671.302 Total estimated model params size (MB)
LLSUB_RANK=4: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/loggers/csv_logs.py:188: UserWarning: Experiment logs directory csv_log/prompt2_cat2/version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
LLSUB_RANK=4:   rank_zero_warn(
LLSUB_RANK=4: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
LLSUB_RANK=4:   rank_zero_warn(
LLSUB_RANK=4: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
LLSUB_RANK=4:   warning_cache.warn(
LLSUB_RANK=4: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
LLSUB_RANK=4:   rank_zero_warn(
LLSUB_RANK=4: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('global_stLLSUB_RANK=5: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
LLSUB_RANK=5:   rank_zero_warn(
LLSUB_RANK=5: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 model_training.py --prompt prompt2_cat3 ...
LLSUB_RANK=5:   rank_zero_warn(
LLSUB_RANK=5: Using 16bit Automatic Mixed Precision (AMP)
LLSUB_RANK=5: GPU available: True (cuda), used: True
LLSUB_RANK=5: TPU available: False, using: 0 TPU cores
LLSUB_RANK=5: IPU available: False, using: 0 IPUs
LLSUB_RANK=5: HPU available: False, using: 0 HPUs
LLSUB_RANK=5: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 model_training.py --prompt prompt2_cat3 ...
LLSUB_RANK=5:   rank_zero_warn(
LLSUB_RANK=5: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-4f1276a4-58eb-2597-9e2a-c281dc35b16d]
LLSUB_RANK=5: 
LLSUB_RANK=5:   | Name              | Type                   | Params
LLSUB_RANK=5: -------------------------------------------------------------
LLSUB_RANK=5: 0 | model             | DiffusionWrapper       | 865 M 
LLSUB_RANK=5: 1 | first_stage_model | AutoencoderKL          | 83.7 M
LLSUB_RANK=5: 2 | cond_stage_model  | FrozenOpenCLIPEmbedder | 354 M 
LLSUB_RANK=5: 3 | control_model     | ControlNet             | 364 M 
LLSUB_RANK=5: -------------------------------------------------------------
LLSUB_RANK=5: 1.2 B     Trainable params
LLSUB_RANK=5: 437 M     Non-trainable params
LLSUB_RANK=5: 1.7 B     Total params
LLSUB_RANK=5: 6,671.302 Total estimated model params size (MB)
LLSUB_RANK=5: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/loggers/csv_logs.py:188: UserWarning: Experiment logs directory csv_log/prompt2_cat3/version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
LLSUB_RANK=5:   rank_zero_warn(
LLSUB_RANK=5: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
LLSUB_RANK=5:   rank_zero_warn(
LLSUB_RANK=5: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
LLSUB_RANK=5:   warning_cache.warn(
LLSUB_RANK=5: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
LLSUB_RANK=5:   rank_zero_warn(
LLSUB_RANK=5: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('global_stLLSUB_RANK=1: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
LLSUB_RANK=1:   rank_zero_warn(
LLSUB_RANK=1: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 model_training.py --prompt prompt1_longest ...
LLSUB_RANK=1:   rank_zero_warn(
LLSUB_RANK=1: Using 16bit Automatic Mixed Precision (AMP)
LLSUB_RANK=1: GPU available: True (cuda), used: True
LLSUB_RANK=1: TPU available: False, using: 0 TPU cores
LLSUB_RANK=1: IPU available: False, using: 0 IPUs
LLSUB_RANK=1: HPU available: False, using: 0 HPUs
LLSUB_RANK=1: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 model_training.py --prompt prompt1_longest ...
LLSUB_RANK=1:   rank_zero_warn(
LLSUB_RANK=1: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-6bc3ef96-b85d-308c-2b58-442956fdd761]
LLSUB_RANK=1: 
LLSUB_RANK=1:   | Name              | Type                   | Params
LLSUB_RANK=1: -------------------------------------------------------------
LLSUB_RANK=1: 0 | model             | DiffusionWrapper       | 865 M 
LLSUB_RANK=1: 1 | first_stage_model | AutoencoderKL          | 83.7 M
LLSUB_RANK=1: 2 | cond_stage_model  | FrozenOpenCLIPEmbedder | 354 M 
LLSUB_RANK=1: 3 | control_model     | ControlNet             | 364 M 
LLSUB_RANK=1: -------------------------------------------------------------
LLSUB_RANK=1: 1.2 B     Trainable params
LLSUB_RANK=1: 437 M     Non-trainable params
LLSUB_RANK=1: 1.7 B     Total params
LLSUB_RANK=1: 6,671.302 Total estimated model params size (MB)
LLSUB_RANK=1: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/loggers/csv_logs.py:188: UserWarning: Experiment logs directory csv_log/prompt1_longest/version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
LLSUB_RANK=1:   rank_zero_warn(
LLSUB_RANK=1: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
LLSUB_RANK=1:   rank_zero_warn(
LLSUB_RANK=1: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
LLSUB_RANK=1:   warning_cache.warn(
LLSUB_RANK=1: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
LLSUB_RANK=1:   rank_zero_warn(
LLSUB_RANK=1: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('LLSUB_RANK=0: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
LLSUB_RANK=0:   rank_zero_warn(
LLSUB_RANK=0: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 model_training.py --prompt prompt1_random ...
LLSUB_RANK=0:   rank_zero_warn(
LLSUB_RANK=0: Using 16bit Automatic Mixed Precision (AMP)
LLSUB_RANK=0: GPU available: True (cuda), used: True
LLSUB_RANK=0: TPU available: False, using: 0 TPU cores
LLSUB_RANK=0: IPU available: False, using: 0 IPUs
LLSUB_RANK=0: HPU available: False, using: 0 HPUs
LLSUB_RANK=0: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 model_training.py --prompt prompt1_random ...
LLSUB_RANK=0:   rank_zero_warn(
LLSUB_RANK=0: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-29d5d050-b350-2001-863b-a5b61589f0ef]
LLSUB_RANK=0: 
LLSUB_RANK=0:   | Name              | Type                   | Params
LLSUB_RANK=0: -------------------------------------------------------------
LLSUB_RANK=0: 0 | model             | DiffusionWrapper       | 865 M 
LLSUB_RANK=0: 1 | first_stage_model | AutoencoderKL          | 83.7 M
LLSUB_RANK=0: 2 | cond_stage_model  | FrozenOpenCLIPEmbedder | 354 M 
LLSUB_RANK=0: 3 | control_model     | ControlNet             | 364 M 
LLSUB_RANK=0: -------------------------------------------------------------
LLSUB_RANK=0: 1.2 B     Trainable params
LLSUB_RANK=0: 437 M     Non-trainable params
LLSUB_RANK=0: 1.7 B     Total params
LLSUB_RANK=0: 6,671.302 Total estimated model params size (MB)
LLSUB_RANK=0: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/lightning_fabric/loggers/csv_logs.py:188: UserWarning: Experiment logs directory csv_log/prompt1_random/version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!
LLSUB_RANK=0:   rank_zero_warn(
LLSUB_RANK=0: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
LLSUB_RANK=0:   rank_zero_warn(
LLSUB_RANK=0: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
LLSUB_RANK=0:   warning_cache.warn(
LLSUB_RANK=0: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
LLSUB_RANK=0:   rank_zero_warn(
LLSUB_RANK=0: /state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('gloep', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.
LLSUB_RANK=6:   warning_cache.warn(
LLSUB_RANK=6: `Trainer.fit` stopped: `max_epochs=10` reached.
ep', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.
LLSUB_RANK=4:   warning_cache.warn(
LLSUB_RANK=4: `Trainer.fit` stopped: `max_epochs=10` reached.
g('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.
LLSUB_RANK=3:   warning_cache.warn(
LLSUB_RANK=3: `Trainer.fit` stopped: `max_epochs=10` reached.
bal_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.
LLSUB_RANK=2:   warning_cache.warn(
LLSUB_RANK=2: `Trainer.fit` stopped: `max_epochs=10` reached.
, ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.
LLSUB_RANK=7:   warning_cache.warn(
LLSUB_RANK=7: `Trainer.fit` stopped: `max_epochs=10` reached.
global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.
LLSUB_RANK=1:   warning_cache.warn(
LLSUB_RANK=1: `Trainer.fit` stopped: `max_epochs=10` reached.
bal_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.
LLSUB_RANK=0:   warning_cache.warn(
LLSUB_RANK=0: `Trainer.fit` stopped: `max_epochs=10` reached.
ep', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.
LLSUB_RANK=5:   warning_cache.warn(
LLSUB_RANK=5: `Trainer.fit` stopped: `max_epochs=10` reached.
