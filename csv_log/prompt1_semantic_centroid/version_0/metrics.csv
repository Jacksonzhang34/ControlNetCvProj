train/loss_simple_step,train/loss_vlb_step,train/loss_step,global_step,epoch,step
0.028312738984823227,0.00010780992306536064,0.028312738984823227,49.0,0,49
0.13415060937404633,0.0008143053273670375,0.13415060937404633,99.0,0,99
0.05357210338115692,0.00018496575648896396,0.05357210338115692,149.0,0,149
0.14036956429481506,0.0007068868144415319,0.14036956429481506,199.0,0,199
0.2909242510795593,0.0016013812273740768,0.2909242510795593,249.0,0,249
0.07781641185283661,0.00026050780434161425,0.07781641185283661,299.0,0,299
0.07684498280286789,0.000285664398688823,0.07684498280286789,349.0,0,349
0.12792950868606567,0.0006316424696706235,0.12792950868606567,399.0,0,399
0.16400901973247528,0.003053166437894106,0.16400901973247528,449.0,0,449
0.08205492794513702,0.0002884106361307204,0.08205492794513702,499.0,0,499
0.06708014756441116,0.0002326055255252868,0.06708014756441116,549.0,0,549
0.08206456154584885,0.0006006070761941373,0.08206456154584885,599.0,0,599
0.21687206625938416,0.03949848562479019,0.21687206625938416,649.0,0,649
0.0522661916911602,0.00017601900617592037,0.0522661916911602,699.0,0,699
0.13406527042388916,0.0014493548078462481,0.13406527042388916,749.0,0,749
0.029311152175068855,0.00010953313176287338,0.029311152175068855,799.0,0,799
0.05002342164516449,0.0001675455569056794,0.05002342164516449,849.0,0,849
0.021237298846244812,8.519857510691509e-05,0.021237298846244812,899.0,0,899
0.20461955666542053,0.009769859723746777,0.20461955666542053,949.0,0,949
