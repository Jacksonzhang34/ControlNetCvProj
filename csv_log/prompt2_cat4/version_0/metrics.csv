train/loss_simple_step,train/loss_vlb_step,train/loss_step,global_step,epoch,step
0.15955229103565216,0.003203888889402151,0.15955229103565216,49.0,0,49
0.16078925132751465,0.001055970904417336,0.16078925132751465,99.0,0,99
0.10949113965034485,0.0005649624508805573,0.10949113965034485,149.0,0,149
0.18124423921108246,0.009916096925735474,0.18124423921108246,199.0,0,199
0.0926423966884613,0.00034341312129981816,0.0926423966884613,249.0,0,249
0.27081218361854553,0.02503046579658985,0.27081218361854553,299.0,0,299
0.106902576982975,0.0004449322004802525,0.106902576982975,349.0,0,349
0.2043047845363617,0.0037369178608059883,0.2043047845363617,399.0,0,399
0.21522098779678345,0.003026495687663555,0.21522098779678345,449.0,0,449
0.06578712165355682,0.00022128276759758592,0.06578712165355682,499.0,0,499
0.3136841654777527,0.00712862703949213,0.3136841654777527,549.0,0,549
0.09477665275335312,0.00036263279616832733,0.09477665275335312,599.0,0,599
0.053465887904167175,0.0001836995070334524,0.053465887904167175,649.0,0,649
0.0707036703824997,0.0002640094899106771,0.0707036703824997,699.0,0,699
0.07664662599563599,0.00035576391383074224,0.07664662599563599,749.0,0,749
0.03584427386522293,0.00012845797755289823,0.03584427386522293,799.0,0,799
0.14233030378818512,0.0006009815842844546,0.14233030378818512,849.0,0,849
0.08080609887838364,0.0005898899398744106,0.08080609887838364,899.0,0,899
0.12334822118282318,0.0004680969286710024,0.12334822118282318,949.0,0,949
